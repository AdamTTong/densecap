===========================================
Linjie Yang 
04/20/2016
===========================================
This directory holds the scripts for image captioning. 
It also holds the scripts for language model learning and paragraph composition using language model.
The main scripts and their usage:
  Image captioning:
    (1) dreamstime_to_hdf5_data.py: preprocessing data. Read an image path list and a caption list, which are aligned, to generate hdf5 files used by model training.
    (2) lrcn.vgg.prototxt: network structure of the model. Reads an image path list and the hdf5 files generated by (1).
    (3) lrcn.vgg.buffer_50.prototxt: same network structure as (2), but this file is specifically for finetuning the network. Note this file uses a batch size of 50, comparing to 100 in (2). We need to use this file because when finetuning, there are much more gradients to store than freezing-cnn training and batch size 100 can no longer fit into the GPU memory.
    (4) lrcn_solver.vgg.prototxt, lrcn_finetune_solver.vgg.prototxt: solver files to train (freezing-cnn) and finetune the network.
    (5) train_lrcn.vgg.sh, finetune_lrcn.vgg.sh: the command file to train and finetune the network.
    (6) run_experiment_vgg_snapchat.py: evaluate the model trained on web data to test on snapchat sample images. Beam search is used to generate the captions. Default beam size is 5.  Note that it uses two separate prototxt files for network description: IMAGE_NET_FILE and LSTM_NET_FILE. You need to modify the two files to make them consistent with the saved model.
    (7) run_experiment_vgg_snapchat_coco.py: evaluate the model trained on coco data to test on snapchat sample images.
    
  Sentence-to-sentence language model:
    (8) books_to_hdf5_data.py: preprocessing data of Books dataset (http://www.cs.toronto.edu/~mbweb/). Similar to (1), except that here is only text data. One sample is a paragraph comprised of several sentences.
    (9) sent_lstm_lm_par.prototxt, lstm_lm_par_solver.prototxt, train_lm_par.sh: network structure, solver, and command file for training the sentence-to-sentence language model.
    (10) gen_lm_samples.py: generate paragraphs with the model from (9). Starting with some pre-defined phrases.
  Sentence-to-sentence language model with skip-thought vectors:
    (11) skip_books_to_hdf5_data.py: preprocessing data. Using skip-thought features as input. One paragraph as a data sample.
    (12) lstm_skip_lm.prototxt, skip_lm_solver.prototxt, train_skip_lm.sh: network structure, solver, and command file for training the model.
    (13) test_composer_beam.py: Using beam search to compose a story using image captions from snapchat stories.
    (14) composer.py: functions to generate a paragraph from sentences. Used by (13)
